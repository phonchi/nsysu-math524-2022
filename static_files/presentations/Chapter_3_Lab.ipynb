{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"6rfz84DL6pzx"},"source":["# Chapter 3 - Linear Regression Laboratory"]},{"cell_type":"code","metadata":{"id":"YKb7d6Cg6pzz"},"source":["import pandas as pd\n","import numpy as np\n","\n","# Modeling\n","from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n","from sklearn import linear_model\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","import statsmodels.formula.api as smf\n","import statsmodels.api as sm\n","from statsmodels.sandbox.regression.predstd import wls_prediction_std\n","from statsmodels.stats.outliers_influence import OLSInfluence\n","from statsmodels.graphics.regressionplots import *\n","from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n","\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","plt.style.use('seaborn-white')\n","sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"txom-Cyx6pz1"},"source":["# temp fix from https://nbviewer.jupyter.org/gist/thatneat/10286720\n","def transform_exog_to_model(fit, exog):\n","    transform=True\n","    self=fit\n","\n","    # The following is lifted straight from statsmodels.base.model.Results.predict()\n","    if transform and hasattr(self.model, 'formula') and exog is not None:\n","        from patsy import dmatrix\n","        exog = dmatrix(self.model.data.orig_exog.design_info.builder,\n","                       exog)\n","\n","    if exog is not None:\n","        exog = np.asarray(exog)\n","        if exog.ndim == 1 and (self.model.exog.ndim == 1 or\n","                               self.model.exog.shape[1] == 1):\n","            exog = exog[:, None]\n","        exog = np.atleast_2d(exog)  # needed in count model shape[1]\n","\n","    # end lifted code\n","    return exog"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZeWASVzv6pz1"},"source":["## Simple Linear Regression\n","\n","The `ISLR2` contains the `Boston`  data set, which records `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status)."]},{"cell_type":"code","metadata":{"id":"hJ989w6LABo7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtSgdIxT6pz2"},"source":["Boston = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/Boston.csv\", index_col='Unnamed: 0')\n","Boston.index = Boston.index - 1 \n","Boston.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ta8W3rsI6pz2"},"source":["print(Boston.shape)\n","print(Boston.info())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PO1bcoQo6pz3"},"source":["We will start by using the `ols()` function to fit a simple  linear regression model, with `medv` as the response and `lstat`  as the predictor."]},{"cell_type":"markdown","metadata":{"id":"wKHx4wyn6pz3"},"source":[" The basic syntax is $ols(y \\sim x, data)$, where `y` is the response, `x` is the predictor, and `data` is the data set in which these two variables are kept."]},{"cell_type":"code","metadata":{"id":"3JH5ml-36pz4"},"source":["# est = smf.ols(y ~ x, data)\n","est = smf.ols('medv ~ lstat',data = Boston).fit()\n","print(est.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQR5zmhXQAn2"},"source":["Another way is to use scikit-learn like API as follows:"]},{"cell_type":"code","metadata":{"id":"YB9-itRDP5vw"},"source":["X = Boston[\"lstat\"]\n","X = sm.add_constant(X)\n","y = Boston[\"medv\"]\n","model = sm.OLS(y,X).fit()\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dILz8tSj6pz4"},"source":["When `statsmodel` detected as a categorical variable, and thus each of its different values are treated as different entities.\n","An integer column can be forced to be treated as categorical using:\n","`\n","model = ols('VIQ ~ C(Gender)', data).fit()\n","`\n","By default, statsmodels treats a categorical variable with `K` possible values as `K-1` ‘dummy’ boolean variables (the last level being absorbed into the intercept term). This is almost always a good default choice - however, it is possible to specify different encodings for categorical variables (http://statsmodels.sourceforge.net/devel/contrasts.html)."]},{"cell_type":"markdown","metadata":{"id":"A0gGYSU06pz5"},"source":["In order to obtain a confidence interval for the coefficient estimates, we can use the `conf_int()` command."]},{"cell_type":"code","metadata":{"id":"NLhUDJMQ6pz5"},"source":["est.conf_int(alpha=0.05)      # default alpha=0.05 : 95% confidence interval"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `predict()` function can be used to produce the prediction for new instance."],"metadata":{"id":"rZ5f046jNU5Q"}},{"cell_type":"code","metadata":{"id":"czA4UAFl6pz6"},"source":["X_new = pd.DataFrame({'lstat':[5,10,15]})\n","est.predict(X_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tYgWy8U6pz6"},"source":["# prediction interval: _, lower bound, upper bound\n","transformed = transform_exog_to_model(est, X_new)\n","wls_prediction_std(est, transformed , weights=[1])[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIBzC_9P6pz6"},"source":["The `get_prediction()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`."]},{"cell_type":"code","metadata":{"id":"O8u4lcbA6pz6"},"source":["predictions = est.get_prediction(X_new)\n","predictions.summary_frame(alpha=0.05)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xi9K41nN6pz7"},"source":["For instance, the 95\\,\\% confidence interval associated with a `lstat` value of 10 is $(24.47, 25.63)$, and the 95\\,\\% prediction interval is $(12.828, 37.28)$.\n","As expected, the confidence and prediction intervals are centered around the same point (a predicted value of $25.05$ for `medv` when `lstat` equals 10), but the latter are substantially wider.\n","\n","We will now plot `medv` and `lstat` along with the least squares regression line using `matplotlib` or `regplot()` functions."]},{"cell_type":"code","metadata":{"id":"71VizVzK6pz7"},"source":["sns.scatterplot(x='lstat', y='medv', data=Boston)\n","\n","X = pd.DataFrame({'lstat':[Boston.lstat.min(), Boston.lstat.max()]})\n","Y_pred = est.predict(X)\n","sns.lineplot(x=X.values[:,0], y=Y_pred.values, color='red')\n","plt.xlabel(\"lstat\")\n","plt.ylabel(\"medv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_DEDEH56pz7"},"source":["sns.regplot(x='lstat',y='medv', data=Boston)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7F78smO6pz8"},"source":["Next we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are plotted according to the results from `ols()`. Also check https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.OLSInfluence.html and https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults"]},{"cell_type":"code","metadata":{"id":"a8B1yRvh6pz8"},"source":["infulence = OLSInfluence(est)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4BsK1e76pz8"},"source":["ols_sm_resid = est.resid # residuals\n","ols_fitted = est.fittedvalues\n","prstd = wls_prediction_std(est)[0]\n","ols_sm_resid_stud = ols_sm_resid / prstd # studentized residuals or infulence.resid_studentized_internal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zos2i_ds6pz9"},"source":["infulence = OLSInfluence(est)\n","ols_sm_resid = est.resid # residuals\n","ols_fitted = est.fittedvalues\n","ols_sm_resid_stud = infulence.resid_studentized_internal\n","leverage = OLSInfluence(est).hat_matrix_diag\n","\n","\n","f, axes = plt.subplots(2, 2, sharex=False, sharey=False) \n","f.set_figheight(15)\n","f.set_figwidth(20)\n","\n","sns.regplot(x='lstat', y='medv', data=Boston, ax=axes[0, 0], scatter_kws={'alpha': 0.5}) # regression plot\n","axes[0, 0].set_title(\"reg plot\")\n","sns.scatterplot(x=ols_fitted,y=ols_sm_resid, ax=axes[0, 1], alpha=0.5)\n","axes[0, 1].set_xlabel(\"fittedvalues\")\n","axes[0, 1].set_ylabel(\"residual\")\n","axes[0, 1].set_title(\"residual plot\")\n","#sns.residplot(x=est.predict(), y='medv', data=df, ax=axes[0, 1], scatter_kws={'alpha': '0.5'}) # residual plot\n","\n","#plot_leverage_resid2(ols_sm_results, ax=axes[1, 0], color='red') # leverage plot\n","\n","# custom leverage plot instead of above\n","#axes[1, 0].autoscale(enable=True, axis='y', tight=True)\n","axes[1, 0].scatter(leverage, ols_sm_resid_stud, alpha=0.5, color='red')\n","axes[1, 0].set_xlabel(\"Leverage\")\n","axes[1, 0].set_ylabel(\"Studentized residuals\")\n","#axes[1, 0].set_ylim(-5, 5)\n","axes[1, 0].set_title(\"leverage\")\n","# studentized residual plot\n","axes[1, 1].scatter(ols_fitted, ols_sm_resid_stud, alpha=0.5, color='magenta')\n","axes[1, 1].axhline(0, ls=\":\", c=\".2\")\n","axes[1, 1].axhline(-3, ls=\":\", c=\".6\")\n","axes[1, 1].axhline(3, ls=\":\", c=\".6\")\n","axes[1, 1].set_ylim(-5, 5)\n","axes[1, 1].set_xlabel(\"fittedvalues\")\n","axes[1, 1].set_ylabel(\"Studentized residuals\")\n","axes[1, 1].set_title(\"studentized residual plot\")\n","\n","x = est.fittedvalues[np.logical_or(ols_sm_resid_stud > 3, ols_sm_resid_stud < -3)]\n","y = ols_sm_resid_stud[np.logical_or(ols_sm_resid_stud > 3, ols_sm_resid_stud < -3)]\n","\n","for i, x, y in zip(x.index, x, y):\n","    axes[1, 1].annotate(i, xy=(x, y));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zMKBC4zAVlAg"},"source":["### Optional - Other useful plot\n","Seaborn also has the functionality of residual plot\n"]},{"cell_type":"code","metadata":{"id":"ANcfcoy4VoCA"},"source":["sns.residplot(x=\"lstat\", y=\"medv\", data=Boston)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDfadY1NSHmZ"},"source":["\n","\n","Statsmodel has more diagonostic plot, like the influence plot where the size of the points is relate to Cook's distance. https://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html"]},{"cell_type":"code","metadata":{"id":"waDJEhGcQuoX"},"source":["f = sm.graphics.influence_plot(est, criterion=\"cooks\")\n","f.set_figheight(10)\n","f.set_figwidth(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghAxnXLAUWvX"},"source":["The `plot_regress_exog` function is a convenience function that gives a 2x2 plot containing the dependent variable and fitted values with confidence intervals vs. the independent variable chosen, the residuals of the model vs. the chosen independent variable, a partial regression plot, and a CCPR plot. This function can be used for quickly checking modeling assumptions with respect to a single regressor. Check https://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html#Component-Component-plus-Residual-(CCPR)-Plots"]},{"cell_type":"code","metadata":{"id":"Wy5aX84lUogf"},"source":["f = sm.graphics.plot_regress_exog(est, \"lstat\")\n","f.set_figheight(10)\n","f.set_figwidth(15)\n","f.tight_layout(pad=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3bsE4K66pz9"},"source":["## Multiple Regression"]},{"cell_type":"markdown","metadata":{"id":"08JKjQ0u6pz9"},"source":["In order to fit a multiple linear regression model using least squares, we again use the `ols()` function. The syntax $ols(y \\sim x1 + x2 + x3)$ is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors."]},{"cell_type":"code","metadata":{"id":"mMNpQYR86pz9"},"source":["#string_cols = ' + '.join(data.columns[:-1])\n","est = smf.ols('medv ~ lstat+age',data = Boston).fit()\n","print(est.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rp5IkPET6pz-"},"source":["The `Boston` data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors.\n","Instead, we can use the code:"]},{"cell_type":"code","metadata":{"id":"J1owqTMq6pz-"},"source":["columns_selected = \"+\".join(Boston.columns.difference([\"medv\"]))\n","my_formula = \"medv ~ \" + columns_selected\n","est = smf.ols(my_formula,data = Boston).fit()\n","print(est.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63N_rVgr6pz-"},"source":["We can access the individual components of a summary object by name. Hence `est.rsquared` gives us the $R^2$.\n","The `vif()` function can be used to compute variance inflation factors.  Most VIF's are low to moderate for this data. Check https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html"]},{"cell_type":"code","metadata":{"id":"kWiSy_R96pz-"},"source":["est.rsquared"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8caSxJZ6pz-"},"source":["# don't forget to add constant if the ols model includes intercept\n","boston = Boston.drop('medv', axis=1).assign(const=1)\n","boston.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qBn91EBW6pz-"},"source":["# variance inflation factors\n","\n","for i, col in enumerate(boston.columns):\n","    if col == 'const':\n","        pass\n","    elif len(col) > 6:\n","        print(col, ':', \"{0:.2f}\".format(vif(boston.to_numpy(), i)))\n","    else:\n","        print(col, '\\t:', \"{0:.2f}\".format(vif(boston.to_numpy(), i)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rrGG7Zkn6pz-"},"source":["What if we would like to perform a regression using all of the variables but one?  For example, in the above regression output,  `age` has a high $p$-value. So we may wish to run a regression excluding this predictor. The following procedure results in a regression using all predictors except `age`."]},{"cell_type":"code","metadata":{"id":"UnCICLZ56pz_"},"source":["columns_selected = \"+\".join(Boston.columns.difference([\"medv\", \"age\"]))\n","my_formula = \"medv ~ \" + columns_selected\n","lm_fit1 = smf.ols(formula = my_formula, data=Boston).fit()\n","lm_fit1.summary().tables[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04wTebwh6pz_"},"source":["## Interaction term"]},{"cell_type":"markdown","metadata":{"id":"KbQky38T6pz_"},"source":["It is easy to include interaction terms in a linear model using the `ols()` function. The syntax `lstat:age` tells `Python` to include an interaction term between `lstat` and `age`. The syntax `lstat * age` simultaneously includes `lstat`, `age`, and the interaction term `lstat`$\\times$`age` as predictors; it is a shorthand for `lstat + age + lstat:age`."]},{"cell_type":"code","metadata":{"id":"SpzgnEOL6pz_"},"source":["est = smf.ols('medv ~ lstat*age',data = Boston).fit()\n","print(est.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIpmwb7d6pz_"},"source":["## Non-linear Transformations of the Predictors"]},{"cell_type":"markdown","metadata":{"id":"tq4RjKef6pz_"},"source":["The `ols()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor $X$, we can create a predictor $X^2$ using  `I(X**2)`. The function `I()` is needed since the `**` has a special meaning in a formula object. We now perform a regression of `medv` onto `lstat` and `lstat^2`."]},{"cell_type":"code","metadata":{"id":"PavTmMix6pz_"},"source":["#adding power term\n","est = smf.ols('medv ~ lstat + I(lstat**2)',data = Boston).fit()\n","print(est.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1CEviOG6pz_"},"source":["The near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit."]},{"cell_type":"code","metadata":{"id":"2tRDzKqp6p0A"},"source":["est2 = smf.ols('medv ~ lstat', data = Boston).fit()\n","sm.stats.anova_lm(est2, est, typ=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXZl7O7D6p0A"},"source":["Here Model 0 represents the linear submodel containing only one predictor, `lstat`, while Model 1 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat^2`. The `anova()` function performs a hypothesis test\n","comparing the two models. The  null hypothesis is that the two models fit the data equally well,  and the alternative hypothesis is that the full model is superior. Here the $F$-statistic is $135$  and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type"]},{"cell_type":"markdown","metadata":{"id":"mqHzAHCY6p0A"},"source":["In order to create a cubic fit, we can include a predictor of the form `I(X**3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `PolynomialFeatures()` function to create the polynomial within `ols()`. For example, the following command produces a fifth-order polynomial fit:"]},{"cell_type":"code","metadata":{"id":"BmPnTnsa6p0A"},"source":["polynomial_features= PolynomialFeatures(degree=5) # using sklearn\n","xp = polynomial_features.fit_transform(Boston.lstat.values.reshape(-1,1))[:,1:] #the intercept should be removed first"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"woS4CmRk6p0A"},"source":["ols_smf = smf.ols(formula='medv ~ xp', data=Boston)\n","ols_smf_results = ols_smf.fit()\n","print(ols_smf_results.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G80LBfzk6p0A"},"source":["This suggests that including additional  polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant $p$-values\n","in a regression fit."]},{"cell_type":"markdown","metadata":{"id":"MudjiF136p0B"},"source":["Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation."]},{"cell_type":"code","metadata":{"id":"BsPmMVgN6p0B"},"source":["# polynomial ols model with intercept\n","ols_smf = smf.ols(formula='medv ~ np.log(rm)', data=Boston)\n","\n","# fitted model and summary\n","ols_smf_results = ols_smf.fit()\n","print(ols_smf_results.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1miAnpp6p0B"},"source":["## Qualitative predictors"]},{"cell_type":"markdown","metadata":{"id":"bnMLSh5M6p0B"},"source":["We will now examine the `Carseats` data, which is part of the `ISLR2`. We will  attempt to predict `Sales`(child car seat sales) in $400$ locations based on a number of predictors."]},{"cell_type":"code","metadata":{"id":"gBH5OL2-6p0B"},"source":["Carseats = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/Carseats.csv\")\n","print(Carseats.shape)\n","Carseats.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"exRQlSB96p0B"},"source":["The `Carseats` data includes qualitative predictors such as `shelveloc`, an indicator of the quality of the shelving location---that is, the  space within a store in which the car seat is displayed---at each location. The predictor `shelveloc` takes on three possible values:  *Bad*, *Medium*, and *Good*. Given a qualitative variable such as `shelveloc`, `Python` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms. The syntax `X1:XP` tells `Python` to include an interaction term between `X1` and `XP`."]},{"cell_type":"code","metadata":{"id":"LPG6pluK6p0B"},"source":["# ols model with intercept\n","columns_selected = \"+\".join(Carseats.columns.difference([\"Sales\"]))\n","my_formula = \"Sales ~ Income:Advertising + Price:Age + \" + columns_selected  \n","\n","# fitted model and summary\n","lm_fit = smf.ols(my_formula, data=Carseats).fit()\n","print(lm_fit.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpEyEI5H6p0B"},"source":["`Python` has created a `ShelveLoc[T.Good]` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLoc[T.Medium]` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables.\n","The fact that the coefficient for `ShelveLoc[T.Good]` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLoc[T.Medium]` has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location."]},{"cell_type":"markdown","metadata":{"id":"2c1aBcbbYYwH"},"source":["Also check `pd.get_dummies` (https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)"]},{"cell_type":"markdown","metadata":{"id":"7EcIvdNj6p0C"},"source":["## The sklearn is another popular way for performing OLS in Python"]},{"cell_type":"markdown","metadata":{"id":"fTU2MFUq6p0C"},"source":["Check `sklearn` (https://scikit-learn.org/stable/modules/linear_model.html)"]},{"cell_type":"code","metadata":{"id":"i7Bj-Zja6p0C"},"source":["# ols model with intercept\n","ols_sl = linear_model.LinearRegression(fit_intercept=True) \n","\n","# fitted ols model (.values.reshape(-1, 1) is required for single predictor?)\n","x_train = Boston.lstat.values.reshape(-1, 1)\n","y_true =  Boston.medv\n","ols_sl.fit(x_train, y_true)\n","\n","y_pred = ols_sl.predict(x_train)\n","# summary\n","ols_sl.intercept_, ols_sl.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rAJ_AhVJ6p0C"},"source":["residual = y_true - y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hipx6EAm6p0C"},"source":["ax = Boston.plot.scatter(x='lstat', y='medv', figsize=(8, 8))\n","ax.plot(Boston.lstat, y_pred)\n","for x, yactual, yfitted in zip(Boston.lstat, Boston.medv, y_pred): \n","    ax.plot((x, x), (yactual, yfitted), '--', color='C1')\n","\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mzSPkpN6p0C"},"source":["ols_sl_summary = {'R2': r2_score(y_true, y_pred), \n","                  'Ex. Var': explained_variance_score(y_true, y_pred), \n","                  'MSE': mean_squared_error(y_true, y_pred)}\n","\n","for k, v in ols_sl_summary.items():\n","    print(k, ':', v)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"klyyhsqr6p0C"},"source":["# out-of-sample predictions\n","ols_sl.predict(np.array([5, 10, 15]).reshape(-1, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6dwhmB3iBT-"},"source":["\n","### Optional - Visualizer for sklearn\n","Sklearn do not come with statistical visulizer like seaborn but you can use yellowbrick (https://www.scikit-yb.org/en/latest/)"]},{"cell_type":"code","metadata":{"id":"EGeHceC_mQUY"},"source":["!pip install -U yellowbrick #besure to upgrade your yellowbrick to above 1.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dk9mSkxKh_Am"},"source":["from yellowbrick.regressor import PredictionError, ResidualsPlot, CooksDistance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YmW8jGBPiR90"},"source":["model = linear_model.LinearRegression(fit_intercept=True)\n","visualizer = PredictionError(model)\n","visualizer.fit(x_train, y_true)  # Fit the training data to the visualizer\n","visualizer.score(x_train, y_true)\n","visualizer.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWzEkv4zkXsS"},"source":["visualizer = ResidualsPlot(model, is_fitted=True)\n","visualizer.score(x_train, y_true)  # Evaluate the model on the test data\n","visualizer.show()                 # Finalize and render the figure"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMqeQ_1SrS9W"},"source":["Histogram can be replaced with a Q-Q plot, which is a common way to check that residuals are normally distributed. If the residuals are normally distributed, then their quantiles when plotted against quantiles of normal distribution should form a straight line. "]},{"cell_type":"code","metadata":{"id":"pFGviP06nbzF"},"source":["plt.figure(figsize=(12,8)) \n","visualizer = ResidualsPlot(model, hist=False, qqplot=True, is_fitted=True)\n","visualizer.score(x_train, y_true)  # Evaluate the model on the test data\n","visualizer.show() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4cZ8rlHnxuN"},"source":["visualizer = CooksDistance()\n","visualizer.fit(x_train, y_true)\n","visualizer.show()"],"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"markdown","metadata":{"id":"XsMVgoYrbd0m"},"source":["# Chapter 8 - Tree-Based Methods"]},{"cell_type":"markdown","metadata":{"id":"h7w2vRTAk2HA"},"source":["#### Student ID:\n","\n","#### Name:"]},{"cell_type":"markdown","metadata":{"id":"Z_99BKJubd0u"},"source":["### Q1:\n","#### For the following pair of figures, where the left figure is a partition of two dimensional feature space and the right one is a decision tree, write down the correct regions $R_1, R_2, \\dots, R_6$ for the (a) to (f), cutpoints $t_1$ and $t_5$ for (g) and (h), and criteria for (i), (j), and (k). "]},{"cell_type":"markdown","source":["&nbsp;&nbsp;$10$&nbsp;|---------------|-------------------------|---------------|  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&emsp;&nbsp;&nbsp;(a)&emsp;&nbsp;&nbsp;|  &emsp;&emsp;&emsp;$X_1 \\leq t_1$  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&nbsp;|  &emsp;&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&nbsp;(i)  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;(g)&nbsp;|---------------|  &emsp;&emsp;|----------|----------|  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&nbsp;&nbsp;&nbsp;(b)&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&nbsp;|  &emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;|  \n","&emsp;&emsp;|&emsp;&nbsp;&nbsp;(c)&emsp;&nbsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&nbsp;|  &emsp;&emsp;|&emsp;&emsp;&nbsp;&nbsp;&nbsp;(j)&emsp;&emsp;|&emsp;&nbsp;&nbsp;&nbsp;(k)  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&emsp;&nbsp;$R_6$&emsp;&nbsp;&nbsp;|  &emsp;&emsp;|&emsp;&emsp;&emsp;|---------|--------|  \n","$X_2$&nbsp;&nbsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&nbsp;|  &emsp;&emsp;|&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;|&emsp;$X_2 \\geq t_5$  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;$t_2$|---------------|----------|---------------|  &emsp;&emsp;|&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&nbsp;|-----|-----|  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&nbsp;&nbsp;&nbsp;$t_3$&emsp;&emsp;&emsp;&nbsp;&nbsp;|  &emsp;&emsp;|&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&nbsp;|&emsp;&emsp;&emsp;|  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;$R_2$&emsp;&nbsp;&nbsp;|&emsp;&emsp;&emsp;(d)&emsp;&emsp;&emsp;|  &emsp;&emsp;|&emsp;&emsp;|---|---|&emsp;&emsp;&nbsp;|&emsp;&emsp;|---|---|  \n","&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;|  &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|  \n","&emsp;$0$&nbsp;|---------------|---------------|--------------------------|  &emsp;&nbsp;&nbsp;$R_1$&nbsp;&nbsp;&nbsp;(e)&emsp;$R_3$&nbsp;&nbsp;&nbsp;$R_4$&emsp;$R_5$&nbsp;&nbsp;&nbsp;(f)  \n","&emsp;&nbsp;&nbsp;&nbsp;$0$&emsp;&emsp;&emsp;&nbsp;&nbsp;(h)&emsp;&nbsp;&emsp;&emsp;&nbsp;$t_4$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$10$  \n","&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$X_1$  \n","&emsp;  "],"metadata":{"id":"qBV9X5H7v9Xk"}},{"cell_type":"markdown","metadata":{"id":"Ia0b99CpnQRW"},"source":["> Ans:  \n","(a)  \n","(b)  \n","(c)  \n","(d)  \n","(e)  \n","(f)  \n","(g)  \n","(h)  \n","(i)  \n","(j)  \n","(k) "]},{"cell_type":"markdown","source":["### Q2:\n","#### Consider the Gini index, classification error, and entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of $p$. The x-axis should display $p$, ranging from $0$ to $1$, and the y-axis should display the value of the Gini index, classification error, and entropy."],"metadata":{"id":"fd02AFE3KdiS"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","p = np.linspace(.001,.999,100)\n","gini = # ___________Gini index___________\n","classerr = # ______classification error______\n","entropy = # _____________entropy_____________\n","\n","fig, ax = plt.subplots()\n","sns.lineplot(x=p, y=gini, color = 'red')\n","sns.lineplot(x=p, y=classerr, color = 'green')\n","sns.lineplot(x=p, y=entropy, color = 'blue')\n","ax.set_xlabel('p')\n","ax.legend(['Gini index', 'classification error', 'entropy'])\n","plt.show()"],"metadata":{"id":"JzH35-3RKc60"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q3: \n","#### Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of $X$, produce 10 estimates of probability $P(\\text{Class is Red}|X)$:\n","\n","#### $$\n","0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, \\text{and}\\ 0.75.\n","$$\n","\n","#### There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final probability $P(\\text{Class is Red}|X)$ under each of these two approaches? What is the final classification under each of these two approaches?"],"metadata":{"id":"KUfp5E0b-_4z"}},{"cell_type":"markdown","source":["> Ans:  \n"],"metadata":{"id":"YKuxBQEKYA1-"}},{"cell_type":"markdown","source":["### Q4: \n","#### In the lab, a classification tree was applied to the `Carseats` data set after converting `Sales` into a qualitative response variable. Now we will seek to predict `Sales` using regression trees and related approaches, treating the response as a quantitative variable. With the data split into a training set and a test set."],"metadata":{"id":"wbNfUnEGDk7q"}},{"cell_type":"code","source":["# Run this block if you're using Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pcyEeQdhfzxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","path = # ____path for Carseats.csv____\n","Carseats = pd.read_csv(path)\n","Carseats.head()"],"metadata":{"id":"MpddgtqsfyKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","data = Carseats.copy()\n","data['Urban'] = data['Urban'].map({'No':0, 'Yes':1,})\n","data['US'] = data['US'].map({'No':0, 'Yes':1})\n","data['ShelveLoc'] = data['ShelveLoc'].map({'Bad':0, 'Medium':5, 'Good':10})\n","\n","X_train, X_test, y_train, y_test = train_test_split(data.drop('Sales',axis= 1),data['Sales'], test_size = 0.5, random_state = 42)"],"metadata":{"id":"-EFsXLh8OIud","executionInfo":{"status":"ok","timestamp":1669019093742,"user_tz":-480,"elapsed":1011,"user":{"displayName":"周柏呈","userId":"06390788947177930230"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9z_hX-AtWTm"},"source":["#### (a) Fit a regression tree with `random_state = 42` to the training set. Obtain the train MSE, plot the tree, and interpret both results. Report the test MSE you obtain."]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.tree import plot_tree, export_graphviz\n","import graphviz\n","\n","reg_tree = # ______regression tree______\n","reg_tree.fit(X_train, y_train)\n","\n","train_mse = # _______training mse_______\n","\n","# _________plot tree_________\n","\n","print('Training MSE is ', train_mse)"],"metadata":{"id":"QswE_8_UOL4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDn7akjGtgGq"},"source":["> Ans:  \n"]},{"cell_type":"code","source":["test_mse = # ________test mse________\n","print('Training MSE is ', test_mse) "],"metadata":{"id":"qUC3zyKSZNPV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t29ypB3Otknp"},"source":["#### (b) Use 10-fold cross-validation in order to determine the optimal level of tree complexity (`max_leaf_nodes`). Does pruning the tree improve the test MSE?\n","(Set random_state for both k-fold and regression tree to 42) "]},{"cell_type":"code","source":["from sklearn.model_selection import KFold, cross_val_score\n","\n","tree_sizes = range(2, 50)\n","scores = []\n","kf = # ______k-fold______\n","for tree_size in tree_sizes:\n","  reg_tree = # ______regression tree______\n","  train_mse = # _______training mse_______\n","  scores.append(train_mse)\n","\n","fig, ax = plt.subplots(figsize=(6, 4))\n","plt.plot(range(2, 50), scores)\n","plt.plot(scores.index(min(scores))+2, min(scores), marker = 'o', c='r')\n","ax.text(scores.index(min(scores))+2, min(scores)+0.1, scores.index(min(scores))+2, size=12)\n","plt.xlim(2,50)\n","plt.xticks(range(2, 50, 6))\n","plt.xlabel('Max Leaf Nodes')\n","plt.ylabel('10-Fold training MSE')"],"metadata":{"id":"aAO4U-tjUJvE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reg_tree = # ______regression tree with best max leaf nodes______\n","reg_tree.fit(X_train, y_train)\n","\n","test_mse = # ________test mse________\n","print('Test MSE is ', test_mse)"],"metadata":{"id":"YL4y68fWis6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVtP6dKotmgX"},"source":["> Ans:  \n"]},{"cell_type":"markdown","source":["---\n","\n"],"metadata":{"id":"ZUN6bBmcepYu"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}